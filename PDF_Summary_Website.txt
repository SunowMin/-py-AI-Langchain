#!pip install langchain
#!pip install streamlit
#!pip install PyPDF2
#!pip install langchain-openai
#pip install langchain-community
#pip install faiss-cpu

import os #íŒŒì´ì¬ì˜ í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ì¸ os ëª¨ë“ˆì„ ì„í¬íŠ¸
from PyPDF2 import PdfReader #PyPDF ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ PdfReader í´ë˜ìŠ¤ ê°–ê³ ì˜´:pdf íŒŒì¼ ì½ê³  í…ìŠ¤íŠ¸ ì¶”ì¶œ
import streamlit as st #streamlit ë¼ì´ë¸ŒëŸ¬ë¦¬ ì „ì²´ë¥¼ stë¼ëŠ” ë³„ì¹­ìœ¼ë¡œ ê°–ê³ ì˜´:streamlitìœ¼ë¡œ ì›¹ì• í”Œë¦¬ì¼€ì´ì…˜ ë§Œë“¤ê¸°
from langchain.text_splitter import CharacterTextSplitter #ë­ì²´ì¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ text_splitter ëª¨ë“ˆì— ìˆëŠ” CharacterTextSplitter í´ë˜ìŠ¤ ì„í¬íŠ¸:í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
from langchain_openai import OpenAIEmbeddings #ë­ì²´ì¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ openai ëª¨ë“ˆì— ìˆëŠ” OpenAIEmbeddings í´ë˜ìŠ¤ ì„í¬íŠ¸ : openaiì˜ ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•´ì„œ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°í™” 
from langchain import FAISS #FAISSë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸:FAISS ë²¡í„° ì €ì¥ì†Œë¥¼ ì‚¬ìš©í•´ ì„ë² ë”©ëœ í…ìŠ¤íŠ¸ë¥¼ ì €ì¥í•˜ê³  ê²€ìƒ‰í•˜ëŠ” ê¸°ëŠ¥ ì œê³µ
from langchain.chains.question_answering import load_qa_chain #ì§ˆë¬¸-ë‹µë³€ ì²´ì¸ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜ : langchain ë¼ì´ë¸ŒëŸ¬ë¦¬ì— chains íŒ¨í‚¤ì§€ì— question_answering ëª¨ë“ˆ ì•ˆì— ì¡´ì¬í•˜ëŠ” load_qa_chain í•¨ìˆ˜ ì„í¬íŠ¸
from langchain.chat_models import ChatOpenAI # ChatOpenAIì˜ GPTëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìì—°ì–´ ì²˜ë¦¬í•˜ëŠ” í´ë˜ìŠ¤ :chat_models ëª¨ë“ˆì˜ ChatOpenAI í´ë˜ìŠ¤ ì„í¬íŠ¸
from langchain.callbacks import get_openai_callback # openai api í˜¸ì¶œ ë¹„ìš©ì„ ì¶”ì í•˜ëŠ” ì½œë°±í•¨ìˆ˜
api_key="sk-(ë³¸ì¸ì˜ Keyê°’)"

def process_text(text): #def í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•´ì„œ process_text() í•¨ìˆ˜ ì •ì˜
#CharacterTextSplitterë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
    text_splitter = CharacterTextSplitter( #CharacterTextSplitter í´ë˜ìŠ¤ì˜ ê°ì²´ë¥¼ ì„¸íŒ…ê°’ê³¼ í•¨ê»˜ ìƒì„±í•˜ì—¬, text_splitter ë³€ìˆ˜ì— ì €ì¥
        separator="\n", # ì²­í¬ ë‚˜ëˆ„ëŠ” ê¸°ì¤€
        chunk_size=1000, # ê° ì²­í¬ì˜ ìµœëŒ€ ê¸¸ì´
        chunk_overlap=200, # ê° ì²­í¬ ê°„ì˜ ì¤‘ë³µ í—ˆìš© ê¸¸ì´ : ë¬¸ë§¥ì„ ìœ ì§€í•˜ê¸° ìœ„í•¨
        length_function=len # ì²­í¬ ê¸¸ì´ ê³„ì‚°ì‹œ len() í•¨ìˆ˜ ì‚¬ìš© ì§€ì •
    )
    
    chunks = text_splitter.split_text(text) # ì•ì—ì„œ ìƒì„±í•œ CharacterTextSplitter í´ë˜ìŠ¤ì˜ ê°ì²´ì¸ text_splitter ì¸ìŠ¤í„´ìŠ¤ì˜ split_text ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•´ì„œ ì¸ìˆ˜ë¡œ ë°›ì€ text ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ chunks ë³€ìˆ˜ì— ì €ì¥(ë¦¬ìŠ¤íŠ¸íƒ€ì…)
# ì˜ˆì‹œ 
# text = "This is a long document that will be split into smaller pieces."
# ì²­í¬ë¡œ ë¶„í• ëœ í…ìŠ¤íŠ¸
# chunks = ["This is a long document", "that will be split", "into smaller pieces."]

    # ì„ë² ë”© ì²˜ë¦¬(ë²¡í„° ë³€í™˜), OpenAIì˜ text-embedding-ada-002 ëª¨ë¸ì„ ì‚¬ìš©    
    embeddings = OpenAIEmbeddings(model="text-embedding-ada-002", api_key=api_key) # ì„ë² ë”© ëª¨ë¸ì„ ì„¤ì •, OpenAIEmbeddings í´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•´ì„œ embeddings ë³€ìˆ˜ì— ì €ì¥    
    documents = FAISS.from_texts(chunks, embeddings) # ì´ë¯¸ ì„¤ì •ëœ embeddings ì¸ìŠ¤í„´ìŠ¤ë¥¼ í™œìš©í•´ ë¶„í• ëœ í…ìŠ¤íŠ¸ ì²­í¬(chrunks)ë¥¼ ì„ë² ë”©(ë²¡í„°)ë¡œ ë³€í™˜í•˜ê³  FAISSë¼ëŠ” ë²¡í„° ê²€ìƒ‰ ì—”ì§„ì— ì €ì¥ 
    return documents # FAISS ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜

def main():  #streamlitì„ ì´ìš©í•œ ì›¹ì‚¬ì´íŠ¸ ìƒì„±
    st.title("ğŸ“„PDF ìš”ì•½í•˜ê¸°") #Streamlit ì›¹ í˜ì´ì§€ì— ì œëª©ì„ í‘œì‹œ
    st.divider() # êµ¬ë¶„ì„  ì¶”ê°€

    pdf = st.file_uploader('PDFíŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”', type='pdf') #streamlit ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ íŒŒì¼ ì—…ë¡œë“œ ìœ„ì ¯ì„ ìƒì„±í•˜ëŠ” ë©”ì„œë“œ

    if pdf is not None: # ì „ì²´ ì½”ë“œê°€ ì‹¤í–‰ëœ í›„ í˜ì´ì§€ê°€ ë Œë”ë§ë˜ê³ , ì‚¬ìš©ìê°€ íŒŒì¼ì„ ì—…ë¡œë“œ í•  ë•Œë§ˆë‹¤ ì „ì²´ ì½”ë“œê°€ ë‹¤ì‹œ ì‹¤í–‰ë¨
        pdf_reader = PdfReader(pdf) # PyPDF2ì˜ PdfReaderë¥¼ ì‚¬ìš©í•˜ì—¬ PDF íŒŒì¼ ì½ì„ PdfReader ê°ì²´ ìƒì„±
        text = ""   # ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ë¥¼ ì €ì¥í•  ë¹ˆ ë¬¸ìì—´ ì´ˆê¸°í™”
        for page in pdf_reader.pages: # pageê°€ pdf_reader.pagesì˜ ê° í•­ëª©ì„ í•˜ë‚˜ì”© ìˆœíšŒ
            text += page.extract_text() # ê° í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì—¬ textë³€ìˆ˜ì— ì¶”ê°€, extract_text():PageObjectì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” ë©”ì†Œë“œ

        documents = process_text(text) # process_text() : #process_text():ìœ„ì— ìƒì„±í•œ í•¨ìˆ˜ ì‹¤í–‰í•˜ê³ , documents ë³€ìˆ˜ì—ëŠ” FAISS ì¸ë±ìŠ¤ ê°ì²´ê°€ ì €ì¥ 
        query = "ì—…ë¡œë“œëœ PDF íŒŒì¼ì˜ ë‚´ìš©ì„ ì•½ 3~5ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”." 

        if query: # queryê°€ ì¡´ì¬í•  ê²½ìš° ì‹¤í–‰, ìœ„ì—ì„œ queryë¥¼ ì‘ì„±í–ˆìœ¼ë‹ˆ ë¬´ì¡°ê±´ ì‹¤í–‰ë¨
            docs = documents.similarity_search(query) # similarity_search() : ì£¼ì–´ì§„ ì¿¼ë¦¬ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œë“¤ì„ ê²€ìƒ‰í•˜ëŠ” ë©”ì„œë“œ
            llm = ChatOpenAI(model="gpt-3.5-turbo-16k", api_key=api_key, temperature=0.1) # LLM ê°ì²´ ìƒì„±
            chain = load_qa_chain(llm, chain_type='stuff') # load_qa_chain() : LangChainì˜ ì§ˆë¬¸-ë‹µë³€ ì²´ì¸ì„ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜

            with get_openai_callback() as cost: # OpenAI API í˜¸ì¶œ ë¹„ìš© ì¶”ì í•˜ê¸° ìœ„í•œ ì½œë°± í•¨ìˆ˜
                response = chain.run(input_documents=docs, question=query) # LLM ì²´ì¸ì„ ì‹¤í–‰í•˜ì—¬ ì…ë ¥ë¬¸ì„œë“¤(docs)ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸(query)ì— ëŒ€í•œ ë‹µë³€(ìš”ì•½)ì„ ìƒì„±
                print(cost) # API í˜¸ì¶œ ë¹„ìš©ì„ ì½˜ì†”ì— ì¶œë ¥

            st.subheader('--ìš”ì•½ ê²°ê³¼--:')
            st.write(response)

if __name__ == '__main__': # íŒŒì´ì¬ ìŠ¤í¬ë¦½íŠ¸ê°€ ì§ì ‘ ì‹¤í–‰ë  ë•Œ ë©”ì¸ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ëŠ” êµ¬ë¬¸
    main() 
